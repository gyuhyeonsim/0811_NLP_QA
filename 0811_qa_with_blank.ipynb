{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BERT model for Question Answering (span extraction).\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the sequence output that computes start_logits and end_logits\n",
    "\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
    "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
    "            into account for computing the loss.\n",
    "        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n",
    "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
    "            into account for computing the loss.\n",
    "\n",
    "    Outputs:\n",
    "        if `start_positions` and `end_positions` are not `None`:\n",
    "            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n",
    "        if `start_positions` or `end_positions` is `None`:\n",
    "            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n",
    "            position tokens of shape [batch_size, sequence_length].\n",
    "\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "    model = BertForQuestionAnswering(config)\n",
    "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from bert_utils.bert_utils import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'language':'kor', 'model_config':123}\n",
    "\n",
    "## logger settings\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if 'kor' in config['language']:\n",
    "    train_file = './datasets/squad_v1.1/train-v1.1.json'\n",
    "    dev_file = './datasets/squad_v1.1/dev-v1.1.json'\n",
    "elif 'eng' in config['language']:\n",
    "    train_file = './datasets/korquad_v1/KorQuAD_v1.0_train.json'\n",
    "    dev_file = './datasets/korquad_v1/KorQuAD_v1.0_dev.json'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_name):\n",
    "    print(\"reading {}\".format(file_name))\n",
    "    with open(file_name, \"r\", encoding='utf-8') as reader:\n",
    "        # >> type(input_data) -> list\n",
    "        input_data = json.load(reader)[\"data\"]\n",
    "    print(\"success to read {}\".format(file_name))\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading ./datasets/squad_v1.1/train-v1.1.json\n",
      "success to read ./datasets/squad_v1.1/train-v1.1.json\n"
     ]
    }
   ],
   "source": [
    "input_data = read_json(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explore the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, dict, dict_keys(['title', 'paragraphs']), list)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(input_data), type(input_data[0]), input_data[0].keys(), type(input_data[0]['paragraphs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Following the disbandment of Destiny\\'s Child in June 2005, she released her second solo album, B\\'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.',\n",
       " 'qas': [{'answers': [{'answer_start': 207, 'text': 'acting'}],\n",
       "   'question': 'After her second solo album, what other entertainment venture did Beyonce explore?',\n",
       "   'id': '56be86cf3aeaaa14008c9076'},\n",
       "  {'answers': [{'answer_start': 369, 'text': 'Jay Z'}],\n",
       "   'question': 'Which artist did Beyonce marry?',\n",
       "   'id': '56be86cf3aeaaa14008c9078'},\n",
       "  {'answers': [{'answer_start': 565, 'text': 'six'}],\n",
       "   'question': 'To set the record for Grammys, how many did Beyonce win?',\n",
       "   'id': '56be86cf3aeaaa14008c9079'},\n",
       "  {'answers': [{'answer_start': 260, 'text': 'Dreamgirls'}],\n",
       "   'question': 'For what movie did Beyonce receive  her first Golden Globe nomination?',\n",
       "   'id': '56bf6e823aeaaa14008c9627'},\n",
       "  {'answers': [{'answer_start': 586, 'text': '2010'}],\n",
       "   'question': 'When did Beyonce take a hiatus in her career and take control of her management?',\n",
       "   'id': '56bf6e823aeaaa14008c9629'},\n",
       "  {'answers': [{'answer_start': 180, 'text': 'Beyoncé'}],\n",
       "   'question': 'Which album was darker in tone from her previous work?',\n",
       "   'id': '56bf6e823aeaaa14008c962a'},\n",
       "  {'answers': [{'answer_start': 406, 'text': 'Cadillac Records'}],\n",
       "   'question': 'After what movie portraying Etta James, did Beyonce create Sasha Fierce?',\n",
       "   'id': '56bf6e823aeaaa14008c962b'},\n",
       "  {'answers': [{'answer_start': 48, 'text': 'June 2005'}],\n",
       "   'question': \"When did Destiny's Child end their group act?\",\n",
       "   'id': '56d43da72ccc5a1400d830bd'},\n",
       "  {'answers': [{'answer_start': 95, 'text': \"B'Day\"}],\n",
       "   'question': \"What was the name of Beyoncé's second solo album?\",\n",
       "   'id': '56d43da72ccc5a1400d830be'},\n",
       "  {'answers': [{'answer_start': 260, 'text': 'Dreamgirls'}],\n",
       "   'question': \"What was Beyoncé's first acting job, in 2006?\",\n",
       "   'id': '56d43da72ccc5a1400d830bf'},\n",
       "  {'answers': [{'answer_start': 369, 'text': 'Jay Z'}],\n",
       "   'question': 'Who is Beyoncé married to?',\n",
       "   'id': '56d43da72ccc5a1400d830c0'},\n",
       "  {'answers': [{'answer_start': 466, 'text': 'Sasha Fierce'}],\n",
       "   'question': \"What is the name of Beyoncé's alter-ego?\",\n",
       "   'id': '56d43da72ccc5a1400d830c1'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[1]['paragraphs'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "    \"\"\"A single training/test example for the Squad dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 qas_id,\n",
    "                 question_text,\n",
    "                 doc_tokens,\n",
    "                 orig_answer_text=None,\n",
    "                 start_position=None,\n",
    "                 end_position=None):\n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.orig_answer_text = orig_answer_text\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"\"\n",
    "        s += \"qas_id: %s\" % (self.qas_id)\n",
    "        s += \"\\n\\n, question_text: %s\" % (\n",
    "            self.question_text)\n",
    "        s += \"\\n\\n, orig_answer_text: %s\" % (\n",
    "            self.orig_answer_text)\n",
    "        s += \"\\n\\n, doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "        if self.start_position:\n",
    "            s += \"\\n\\n, start_position: %d\" % (self.start_position)\n",
    "        if self.start_position:\n",
    "            s += \"\\n\\n, end_position: %d\" % (self.end_position)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습 자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-8260afdadd9f>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-8260afdadd9f>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def parse_json_squad(input_data, is_train):\n",
    "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "    examples = list()\n",
    "    for data_entry in input_data:\n",
    "        for paragraph in data_entry['paragraphs']:\n",
    "            paragraph_text = paragraph[\"context\"]\n",
    "            doc_tokens = []\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "\n",
    "            # Q1. doc_tokens에 whitespace(c)를 가지고 context를 토큰화하는 코드를 작성하세요.\n",
    "            ###################################################################################################\n",
    "            for char in paragraph_text:\n",
    "                if is_whitespace(char):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append( #blank# )\n",
    "                    else:\n",
    "                        doc_tokens[-1] += #blank#\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)  # Which word is the character in?\n",
    "            ###################################################################################################\n",
    "    \n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                \"\"\"\n",
    "                {'answers': [{'answer_start', 'text'}], 'question', 'id'}\n",
    "                \"\"\"\n",
    "                qas_id = qa[\"id\"]\n",
    "                question_text = qa[\"question\"]\n",
    "                start_position = None\n",
    "                end_position = None\n",
    "                orig_answer_text = None\n",
    "\n",
    "                if is_train:\n",
    "                    if len(qa[\"answers\"]) != 1:\n",
    "                        raise ValueError(\n",
    "                            \"For training, each question should have exactly 1 answer.\")\n",
    "                    \n",
    "                    # Q2. Line 34의 변수를 참고하여 Line 70: SquadExample의 instance를 만들기 위한 파라미터를 채우세요.  \n",
    "                    ###################################################################################################\n",
    "                    qas_id = qa[\"#blank#\"]     # fill the black -> assign None\n",
    "                    question_text = qa[\"#blank#\"] # fill the black # index of word\n",
    "                    answer = qa[\"#blank#\"][0]\n",
    "                    orig_answer_text = answer[\"#blank#\"]\n",
    "                    answer_offset = answer[\"#blank#\"]\n",
    "                    answer_length = len(orig_answer_text)\n",
    "                    start_position = char_to_word_offset[answer_offset] # index of word\n",
    "                    end_position = char_to_word_offset[answer_offset + answer_length - 1] # index of word \n",
    "                    ###################################################################################################\n",
    "\n",
    "                    # CODE FOR Handling exceptions \n",
    "                    # Only add answers where the text can be exactly recovered from the\n",
    "                    # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "                    # stuff so we will just skip the example.\n",
    "                    #\n",
    "                    # Note that this means for training mode, every example is NOT\n",
    "                    # guaranteed to be preserved.\n",
    "                    actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
    "                    cleaned_answer_text = \" \".join(whitespace_tokenize(\n",
    "                        orig_answer_text))  # segment words from the sentense including the white space\n",
    "                    if actual_text.find(cleaned_answer_text) == -1:\n",
    "                        logger.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                       actual_text, cleaned_answer_text)\n",
    "                        continue\n",
    "\n",
    "                example = SquadExample(\n",
    "                    qas_id=qas_id,\n",
    "                    question_text=question_text,\n",
    "                    doc_tokens=doc_tokens,  # a set of tokens(words) in the\n",
    "                    orig_answer_text=orig_answer_text,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position)\n",
    "                examples.append(example)\n",
    "    print(\"success to convert input data into a set of {} examples\".format(len(examples)))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = parse_json_squad(input_data, True)\n",
    "## len of examples 87599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_id,\n",
    "                 example_index,\n",
    "                 doc_span_index,\n",
    "                 tokens,\n",
    "                 token_to_orig_map,\n",
    "                 token_is_max_context,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 start_position=None,\n",
    "                 end_position=None):\n",
    "        \n",
    "        self.unique_id = unique_id\n",
    "        self.example_index = example_index\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-16e652569d77>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-16e652569d77>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    tok_end_position = None\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    unique_id = 1000000000\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    for (example_index, example) in enumerate(examples):\n",
    "        \n",
    "        # Q3. Pretrained model을 학습하는데 사용한 Tokenizer를 사용하여 question과 context의 token을 sub-token으로 토큰화해주세요.  \n",
    "        ###################################################################################################\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = [] \n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens: \n",
    "                tok_to_orig_index.append(#blank#)                  \n",
    "                all_doc_tokens.append(#blank#)\n",
    "        ## fill black with the code for appending some values to the above lists\n",
    "        ###################################################################################################\n",
    "                            \n",
    "        # Q4. sub-tokens에 맞추어 span을 업데이트 해주세요.\n",
    "        ###################################################################################################\n",
    "        # fill the code for updating the span of token (tok_start_position, tok_end_position)\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "        if is_training:\n",
    "            tok_start_position = orig_to_tok_index[example.start_position]\n",
    "        if example.end_position < len(example.doc_tokens) - 1:\n",
    "            tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "        else:\n",
    "            tok_end_position = len(all_doc_tokens) - 1\n",
    "        (tok_start_position, tok_end_position) = improve_answer_span(\n",
    "            all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "            example.orig_answer_text)\n",
    "            \n",
    "        ####################################################################################################\n",
    "        \n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = #blank# - len(query_tokens) - 3\n",
    "\n",
    "        # doc_tokens가 설정한 max length를 넘는다면, 몇개의 DocSpan으로 쪼개야 합니다.\n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "            \n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = [] # input data\n",
    "            segment_ids = [] # segment data\n",
    "            token_to_orig_map = {}\n",
    "            token_is_max_context = {}\n",
    "\n",
    "            # Q5. query를 pretrained BERT의 입력값(features) 형식에 따라 바꿔주세요.\n",
    "              # tokens -> [CLS] question [SEP] context [SEP]\n",
    "              # segment_ids -> 00000000000000000000 1111111111111\n",
    "            ###################################################################################################            \n",
    "            \n",
    "            for token in query_tokens:\n",
    "                ## fill the blank ##        \n",
    "            ###################################################################################################\n",
    "\n",
    "                \n",
    "            # Q6. context를 pretrained BERT의 입력값(features) 형식에 따라 바꿔주세요.\n",
    "            ###################################################################################################\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "                is_max_context = check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(1) # segment ids 12 means the context\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "            ###################################################################################################\n",
    "            \n",
    "            # convert into the index of emmeding matrix\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            \n",
    "            # Q7. pretrained BERT의 입력값 크기에 맞게 zero-padding 해주세요.\n",
    "            ###################################################################################################\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "            while len(input_ids) < #blank#:\n",
    "                input_ids.append(#blank#)\n",
    "                input_mask.append(#blank#)\n",
    "                segment_ids.append(0)\n",
    "            ###################################################################################################\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "            \n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            if is_training:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                if (example.start_position < doc_start or\n",
    "                        example.end_position < doc_start or\n",
    "                        example.start_position > doc_end or example.end_position > doc_end):\n",
    "                    continue # -> next to the DocSpan \n",
    "\n",
    "                doc_offset = len(query_tokens) + 2\n",
    "                start_position = tok_start_position - doc_start + doc_offset\n",
    "                end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "            if example_index < 20:\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"unique_id: %s\" % (unique_id))\n",
    "                logger.info(\"example_index: %s\" % (example_index))\n",
    "                logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "                logger.info(\"tokens: %s\" % \" \".join(tokens))\n",
    "                logger.info(\"token_to_orig_map: %s\" % \" \".join([\n",
    "                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
    "                logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
    "                ]))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                logger.info(\n",
    "                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                if is_training:\n",
    "                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "                    logger.info(\"start_position: %d\" % (start_position))\n",
    "                    logger.info(\"end_position: %d\" % (end_position))\n",
    "                    logger.info(\n",
    "                        \"answer: %s\" % (answer_text))\n",
    "                    \n",
    "            features.append(\n",
    "                InputFeatures(\n",
    "                    unique_id=unique_id,\n",
    "                    example_index=example_index,\n",
    "                    doc_span_index=doc_span_index,\n",
    "                    tokens=tokens,\n",
    "                    token_to_orig_map=token_to_orig_map,\n",
    "                    token_is_max_context=token_is_max_context,\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    start_position=start_position,\n",
    "                    end_position=end_position))\n",
    "            unique_id += 1\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap the train_features in a `Dataloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config='bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(config)\n",
    "max_seq_length=128\n",
    "doc_stride=128\n",
    "max_query_length=64\n",
    "cached_train_features_file = train_file+'_{0}_{1}_{2}'.format(config, str(max_seq_length), str(doc_stride))\n",
    "\n",
    "train_batch_size=15\n",
    "predict_batch_size=15\n",
    "num_train_epochs=2\n",
    "gradient_accumulation_steps=1\n",
    "warmup_proportion=0.1\n",
    "learning_rate=5e-5\n",
    "num_train_steps = int(num_train_epochs * len(train_examples) / train_batch_size) #  /gradient_accumulation_steps *num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(cached_train_features_file, \"rb\") as reader:\n",
    "        train_features = pickle.load(reader)\n",
    "except:\n",
    "    train_features = convert_examples_to_features(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length,\n",
    "        doc_stride=doc_stride,\n",
    "        max_query_length=max_query_length,\n",
    "        is_training=True)\n",
    "    print('finish extracting the features from the examples')\n",
    "    logger.info(\"  Saving train features into cached file %s\", cached_train_features_file)\n",
    "    with open(cached_train_features_file, \"wb\") as writer:\n",
    "        pickle.dump(train_features, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num orig examples = %d\", len(train_examples))\n",
    "logger.info(\"  Num split examples = %d\", len(train_features))\n",
    "logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "\n",
    "# Q8. 데이터로더에 데이터를 로드하기 위해 tensor로 data type을 torch.Tensor로 바꿔주세요.\n",
    "###################################################################################################\n",
    "# fill the blank\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype= #blank#) ## must be long type\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype= #blank#)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype= #blank#)\n",
    "all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=#blank# )\n",
    "all_end_positions = torch.tensor([f.end_position for f in train_features], dtype= #blank#)\n",
    "###################################################################################################\n",
    "\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,all_start_positions, all_end_positions)                           \n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(config)\n",
    "local_rank = -1\n",
    "gpu_num =2\n",
    "device = torch.device(f\"cuda:{gpu_num}\")\n",
    "t_total = num_train_steps\n",
    "model.to(device)  # [2] TITAN Xp         | 43'C,   0 % |  1235 / 12196 MB | gyuhyeon(1223M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = num_train_steps\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "# hack to remove pooler, which is not used\n",
    "# thus it produce None grad that break apex\n",
    "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]  # remove the first class label('pooler')\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "t_total = num_train_steps\n",
    "\n",
    "# find BertAdam in the https://huggingface.co/transformers/migration.html\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=learning_rate,\n",
    "                     warmup=warmup_proportion,\n",
    "                     t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "model.train()\n",
    "# Q9. Pretrained BERT를 fine tuning하는 training 코드를 채워주세요.\n",
    "###################################################################################################\n",
    "# [2] TITAN Xp         | 61'C,  65 % |  6757 / 12196 MB | gyuhyeon(6745M)\n",
    "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        # loss.backward()까지의 코드를 채워주세요.\n",
    "        # blank:: batch to device#\n",
    "        input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n",
    "        loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
    "        loss.backward()\n",
    "        \n",
    "        \"\"\"\n",
    "        Added Explaination for the Below Code\n",
    "        f your data set is highly differentiated, you can suffer from a sort of \"early over-fitting\". \n",
    "        If your shuffled data happens to include a cluster of related, strongly-featured observations,\n",
    "        your model's initial training can skew badly toward those features -- or worse,\n",
    "        toward incidental features that aren't truly related to the topic at all.\n",
    "\n",
    "        Warm-up is a way to reduce the primacy effect of the early training examples.\n",
    "        Without it, you may need to run a few extra epochs to get the convergence desired,\n",
    "        as the model un-trains those early superstitions.\n",
    "        \"\"\"\n",
    "        lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr_this_step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        \n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "# output_model_file = os.path.join(\"/Model\", \"pytorch_model.bin\")\n",
    "torch.save(model_to_save.state_dict(), \"./save/models/squad_finetuned_bert_128_epochs_2.bin\")\n",
    "\n",
    "os.listdir('./save/models')\n",
    "\n",
    "\"\"\"\n",
    "torch.save({\"model_type\": self.model_type,\n",
    "            \"start_epoch\": epoch + 1,\n",
    "            \"network\": self.net.state_dict(),\n",
    "            \"optimizer\": self.optim.state_dict(),\n",
    "            \"best_metric\": self.best_metric,\n",
    "            }, str(save_path) + \"/%s.pth.tar\" % (filename))\n",
    "\n",
    "## how to load            \n",
    "self.net.load_state_dict(ckpoint['network'])\n",
    "self.optim.load_state_dict(ckpoint['optimizer'])\n",
    "self.start_epoch = ckpoint['start_epoch']\n",
    "self.best_metric = ckpoint[\"best_metric\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_num =2\n",
    "device = torch.device(f\"cuda:{gpu_num}\")\n",
    "\n",
    "config = 'bert-base-multilingual-cased'\n",
    "model = BertForQuestionAnswering.from_pretrained(config)\n",
    "tokenizer = BertTokenizer.from_pretrained(config)\n",
    "\n",
    "# 10. fine tuned BERT 모델 파라미터를 로드하는 코드를 채워주세요.\n",
    "###################################################################################################\n",
    "saving_point = torch.load(\"./save/models/squad_finetuned_bert_128_epochs_2.bin\")\n",
    "model.#blank#(saving_point)\n",
    "model.to(device)\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=128\n",
    "doc_stride=128\n",
    "max_query_length=64\n",
    "predict_batch_size=50\n",
    "\n",
    "# 11. dev data를 로드하고 전처리 해주세요\n",
    "###################################################################################################\n",
    "input_data = read_json(dev_file)\n",
    "eval_examples = parse_json_squad(\n",
    "    input_data=input_data, is_train=False)\n",
    "eval_features = #blank#(\n",
    "    examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=doc_stride,\n",
    "    max_query_length=max_query_length,\n",
    "    is_training=False)\n",
    "###################################################################################################\n",
    "\n",
    "logger.info(\"***** Running predictions *****\")\n",
    "logger.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Num split examples = %d\", len(eval_features))\n",
    "logger.info(\"  Batch size = %d\",predict_batch_size)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
    "\n",
    "# 12. inference를 위한 데이터로더를 만들어주세요.\n",
    "###################################################################################################\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(#blank#)\n",
    "eval_dataloader = DataLoader(#blank#, sampler=eval_sampler, batch_size=predict_batch_size)\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_results = []\n",
    "logger.info(\"Start evaluating\")\n",
    "for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    if len(all_results) % 1000 == 0:\n",
    "        logger.info(\"Processing example: %d\" % (len(all_results)))\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
    "    for i, example_index in enumerate(example_indices):\n",
    "        start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
    "        end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
    "        eval_feature = eval_features[example_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "        all_results.append(RawResult(unique_id=unique_id,\n",
    "                                     start_logits=start_logits,\n",
    "                                     end_logits=end_logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](images/image_1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "start_logit = all_results[6].start_logits\n",
    "end_logit = all_results[6].end_logits\n",
    "index = np.arange(len(start_logit))\n",
    "\n",
    "# 13. Infer 모드의 model output을 plot해주세요.\n",
    "###################################################################################################\n",
    "# berore softmax\n",
    "\n",
    "\n",
    "# after softmax\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "pred_start_index = np.argmax(start_logit)\n",
    "pred_end_index = np.argmax(end_logit)\n",
    "print(np.argmax(start_logit), np.argmax(end_logit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_start_index = np.argmax(start_logit)\n",
    "pred_end_index = np.argmax(end_logit)\n",
    "orig_doc_start = eval_features[6].token_to_orig_map[pred_start_index]\n",
    "orig_doc_end = eval_features[6].token_to_orig_map[pred_end_index]\n",
    "orig_tokens = eval_examples[eval_features[6].example_index].doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "\n",
    "print(eval_examples[eval_features[6].example_index])\n",
    "print(\"\\n\\nAnswer: {}\".format(\" \".join(orig_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir='./save/predictions'\n",
    "n_best_size=20\n",
    "max_answer_length=30\n",
    "do_lower_case=True\n",
    "verbose_logging=False\n",
    "\n",
    "output_prediction_file = os.path.join(output_dir, \"squad_test_predictions.json\")\n",
    "output_nbest_file = os.path.join(output_dir, \"nbest_squad_test_predictions.json\")\n",
    "all_predictions = write_predictions(eval_examples, eval_features, all_results,\n",
    "                  n_best_size, max_answer_length,\n",
    "                  do_lower_case, output_prediction_file,\n",
    "                  output_nbest_file, verbose_logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./datasets/squad_v1.1/dev-v1.1.json') as f:\n",
    "    dataset_json = json.load(f)\n",
    "    dataset = dataset_json['data']\n",
    "with open('./save/predictions/squad_test_predictions.json') as f:\n",
    "    preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n",
    "has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
    "no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
    "exact, f1 = get_raw_scores(dataset, preds)\n",
    "\n",
    "out_eval = make_eval_dict(exact, f1)\n",
    "out_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KorQuAD v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
